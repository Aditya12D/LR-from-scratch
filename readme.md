# Linear Regression Models from Scratch

This repository contains Python implementations of **Simple Linear Regression** and **Multiple Linear Regression** from scratch, alongside comparisons with scikit-learn models and visualizations for conceptual clarity.

## ğŸ“‚ Files

| File                          | Description                                                                                                                      |
| ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `simplelinearregression.py`   | Implements simple linear regression without libraries and validates with scikit-learn.                                           |
| `multiplelinearregression.py` | Implements multiple linear regression (Normal Equation), compares with scikit-learn, and visualizes regression hyperplane in 3D. |

## ğŸ”§ Features

âœ… Builds regression models **from scratch** (no direct ML libraries)
âœ… Compares results with scikit-learn models
âœ… Visualizes regression line and hyperplane
âœ… Uses real-world datasets with numerical and categorical features

## ğŸ“Š Datasets

1. **Study\_vs\_Score\_data.csv** â€“ For simple regression with:

| Attendance\_Hours | Final\_Marks |
| ----------------- | ------------ |

2. **multiple\_linear\_regression\_dataset.csv** â€“ For multiple regression with:

| age | experience | income |
| --- | ---------- | ------ |

3. **Student\_Performance.csv** â€“ For multiple regression with categorical encoding.

## ğŸ“ˆ Outputs

* **Simple Linear Regression:** Regression line over scatter plot predicting Final Marks from Attendance Hours.
* **Multiple Linear Regression:**

  * Predictions from custom and scikit-learn models
  * 3D regression hyperplane plots for age, experience, and income data
  * Encoded features analysis for student performance

## ğŸ“ Key Learnings

* Deriving slope and intercept for simple linear regression
* Implementing **Normal Equation** for multiple linear regression:

  $$
  \beta = (X^TX)^{-1}X^Ty
  $$
* Using `np.linalg.inv()` for matrix inversion in the Normal Equation solution
* Understanding **matrix differentiation** to derive the Normal Equation, where setting the derivative of the cost function to zero yields the closed-form solution
* Handling categorical features using `OneHotEncoder`
* Visualizing high-dimensional regression models

## âœ¨ Future Improvements

* Implement **gradient descent optimization**
* Extend to **polynomial regression**
* Integrate evaluation metrics (RMSE, RÂ² score) in outputs
