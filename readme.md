# Linear Regression Models from Scratch

This repository contains Python implementations of **Simple Linear Regression** and **Multiple Linear Regression** from scratch, alongside comparisons with scikit-learn models and visualizations for conceptual clarity.

## üìÇ Files

| File                          | Description                                                                                                                      |
| ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `simplelinearregression.py`   | Implements simple linear regression without libraries and validates with scikit-learn.                                           |
| `multiplelinearregression.py` | Implements multiple linear regression (Normal Equation), compares with scikit-learn, and visualizes regression hyperplane in 3D. |

## üîß Features

‚úÖ Builds regression models **from scratch** (no direct ML libraries)
‚úÖ Compares results with scikit-learn models
‚úÖ Visualizes regression line and hyperplane
‚úÖ Uses real-world datasets with numerical and categorical features

## üìä Datasets

1. **Study\_vs\_Score\_data.csv** ‚Äì For simple regression with:

| Attendance\_Hours | Final\_Marks |
| ----------------- | ------------ |

2. **multiple\_linear\_regression\_dataset.csv** ‚Äì For multiple regression with:

| age | experience | income |
| --- | ---------- | ------ |

3. **Student\_Performance.csv** ‚Äì For multiple regression with categorical encoding.

> ‚ö†Ô∏è **Note:** Adjust dataset paths in scripts as per your directory structure.

## üìà Outputs

* **Simple Linear Regression:** Regression line over scatter plot predicting Final Marks from Attendance Hours.
* **Multiple Linear Regression:**

  * Predictions from custom and scikit-learn models
  * 3D regression hyperplane plots for age, experience, and income data
  * Encoded features analysis for student performance

## üìù Key Learnings

* Deriving slope and intercept for simple linear regression
* Implementing **Normal Equation** for multiple linear regression:

  $$
  \beta = (X^TX)^{-1}X^Ty
  $$
* Using `np.linalg.inv()` for matrix inversion in the Normal Equation solution
* Understanding **matrix differentiation** to derive the Normal Equation, where setting the derivative of the cost function to zero yields the closed-form solution
* Handling categorical features using `OneHotEncoder`
* Visualizing high-dimensional regression models

## ‚ú® Future Improvements

* Implement **gradient descent optimization**
* Extend to **polynomial regression**
* Integrate evaluation metrics (RMSE, R¬≤ score) in outputs
